{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from material_parser.core.material_parser import MaterialParserBuilder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bi_fe_ratio', 'separate_hydrolysis', 'precursor_concentration', 'pH', 'stirring_time_hr', 'stirring_temp_degC', 'age_days', 'age_temp_degC', 'low_coating_time_sec', 'low_coating_rpm', 'high_coating_time_sec', 'high_coating_rpm', 'dry_time_min', 'dry_degC', 'layer_prebake_time_min', 'layer_prebake_degC', 'layer_annealing_time_min', 'layer_annealing_degC', 'layers', 'final_prebake_time_min', 'final_prebake_degC', 'final_annealing_time_hr', 'final_annealing_degC', 'nitrate_precs', 'air_atm', 'o2_atm', 'n2_atm', 'chem_pca-c1', 'chem_pca-c2', 'chem_pca-c3', 'chem_pca-c4', 'chem_pca-c5', 'chem_pca-c6', 'chem_pca-c7', 'chem_pca-c8', 'chem_pca-c9', 'chem_pca-c10', 'chem_pca-c11', 'chem_pca-c12', 'chem_pca-c13', 'chem_pca-c14', 'chem_pca-c15', 'chem_pca-c16', 'chem_pca-c17', 'chem_pca-c18', 'chem_pca-c19', 'chem_pca-c20', 'chem_pca-c21', 'chem_pca-c22', 'chem_pca-c23', 'chem_pca-c24', 'chem_pca-c25', 'chem_pca-c26', 'chem_pca-c27', 'chem_pca-c28', 'chem_pca-c29', 'chem_pca-c30']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "df = pd.read_csv('./data/bfo_df_modeling_knn_20220825.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# WARNING: EDIT THIS (among other things)\n",
    "df = df.fillna(0)\n",
    "\n",
    "features_df = df.drop(['Unnamed: 0', 'recipe_id', 'impurity_code'], axis=1)\n",
    "labels = df['impurity_code']\n",
    "\n",
    "feature_list = list(features_df.columns)\n",
    "print(feature_list)\n",
    "features = features_df.to_numpy()\n",
    "labels = labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bi_fe_ratio', 'separate_hydrolysis', 'precursor_concentration', 'pH', 'stirring_time_hr', 'stirring_temp_degC', 'age_days', 'age_temp_degC', 'low_coating_time_sec', 'low_coating_rpm', 'high_coating_time_sec', 'high_coating_rpm', 'dry_time_min', 'dry_degC', 'layer_prebake_time_min', 'layer_prebake_degC', 'layer_annealing_time_min', 'layer_annealing_degC', 'layers', 'final_prebake_time_min', 'final_prebake_degC', 'final_annealing_time_hr', 'final_annealing_degC', 'nitrate_precs', 'air_atm', 'o2_atm', 'n2_atm', 'chem_pca-c1', 'chem_pca-c2', 'chem_pca-c3', 'chem_pca-c4', 'chem_pca-c5', 'chem_pca-c6', 'chem_pca-c7', 'chem_pca-c8', 'chem_pca-c9', 'chem_pca-c10', 'chem_pca-c11', 'chem_pca-c12', 'chem_pca-c13', 'chem_pca-c14', 'chem_pca-c15', 'chem_pca-c16', 'chem_pca-c17', 'chem_pca-c18', 'chem_pca-c19', 'chem_pca-c20', 'chem_pca-c21', 'chem_pca-c22', 'chem_pca-c23', 'chem_pca-c24', 'chem_pca-c25', 'chem_pca-c26', 'chem_pca-c27', 'chem_pca-c28', 'chem_pca-c29', 'chem_pca-c30']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "lit_df = pd.read_csv('./data/bfo_lit_df_modeling_knn_20220825.csv')\n",
    "lit_df = lit_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# WARNING: EDIT THIS (among other things)\n",
    "lit_df = lit_df.fillna(0)\n",
    "\n",
    "lit_features_df = lit_df.drop(['Unnamed: 0', 'recipe_id', 'impurity_code'], axis=1)\n",
    "lit_labels = lit_df['impurity_code']\n",
    "\n",
    "lit_feature_list = list(lit_features_df.columns)\n",
    "print(lit_feature_list)\n",
    "lit_features = lit_features_df.to_numpy()\n",
    "lit_labels = lit_labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bi_fe_ratio', 'separate_hydrolysis', 'precursor_concentration', 'pH', 'stirring_time_hr', 'stirring_temp_degC', 'age_days', 'age_temp_degC', 'low_coating_time_sec', 'low_coating_rpm', 'high_coating_time_sec', 'high_coating_rpm', 'dry_time_min', 'dry_degC', 'layer_prebake_time_min', 'layer_prebake_degC', 'layer_annealing_time_min', 'layer_annealing_degC', 'layers', 'final_prebake_time_min', 'final_prebake_degC', 'final_annealing_time_hr', 'final_annealing_degC', 'nitrate_precs', 'air_atm', 'o2_atm', 'n2_atm', 'chem_pca-c1', 'chem_pca-c2', 'chem_pca-c3', 'chem_pca-c4', 'chem_pca-c5', 'chem_pca-c6', 'chem_pca-c7', 'chem_pca-c8', 'chem_pca-c9', 'chem_pca-c10', 'chem_pca-c11', 'chem_pca-c12', 'chem_pca-c13', 'chem_pca-c14', 'chem_pca-c15', 'chem_pca-c16', 'chem_pca-c17', 'chem_pca-c18', 'chem_pca-c19', 'chem_pca-c20', 'chem_pca-c21', 'chem_pca-c22', 'chem_pca-c23', 'chem_pca-c24', 'chem_pca-c25', 'chem_pca-c26', 'chem_pca-c27', 'chem_pca-c28', 'chem_pca-c29', 'chem_pca-c30']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "suggested_df = pd.read_csv('./data/bfo_only_suggested_df_modeling_knn_20220825.csv')\n",
    "suggested_df = suggested_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# WARNING: EDIT THIS (among other things)\n",
    "suggested_df = suggested_df.fillna(0)\n",
    "\n",
    "suggested_features_df = suggested_df.drop(['Unnamed: 0', 'recipe_id', 'impurity_code'], axis=1)\n",
    "suggested_labels = suggested_df['impurity_code']\n",
    "\n",
    "suggested_feature_list = list(suggested_features_df.columns)\n",
    "print(suggested_feature_list)\n",
    "suggested_features = suggested_features_df.to_numpy()\n",
    "suggested_labels = suggested_labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of oversampled data:  410\n",
      "number of pure syntheses in oversampled data:  205\n",
      "number of impure syntheses in oversampled data:  205\n"
     ]
    }
   ],
   "source": [
    "# Implement SMOTE (Sythetic Minority Oversampling Technique)\n",
    "X = lit_features_df #df.loc[:, df.columns != 'impurity_code']\n",
    "y = lit_labels #df.loc[:, df.columns == 'impurity_code']\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "over_sample = SMOTE(random_state=512)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=512)\n",
    "columns = X_train.columns\n",
    "\n",
    "os_data_X, os_data_y = over_sample.fit_resample(X_train, y_train)\n",
    "\n",
    "#os_data_X = pd.DataFrame(data=os_data_X, columns=columns)\n",
    "\n",
    "print('length of oversampled data: ', len(os_data_X))\n",
    "print('number of pure syntheses in oversampled data: ', len(os_data_y[os_data_y==0]))\n",
    "print('number of impure syntheses in oversampled data: ', len(os_data_y[os_data_y==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    test_labels = test_labels + 1\n",
    "    predictions = model.predict(test_features) + 1\n",
    "    errors = abs(predictions - (test_labels))\n",
    "    \n",
    "#     pprint(predictions)\n",
    "#     pprint(test_labels)\n",
    "    \n",
    "    tp = np.count_nonzero(predictions + test_labels == 4)\n",
    "    tn = np.count_nonzero(predictions + test_labels == 2)\n",
    "    fp = np.count_nonzero(predictions - test_labels == 1)\n",
    "    fn = np.count_nonzero(predictions - test_labels == -1)\n",
    "    \n",
    "    if tp or fp:\n",
    "        precision = tp / (tp + fp)\n",
    "    else:\n",
    "        precision = 0\n",
    "        \n",
    "    if tp or fn:\n",
    "        recall = tp / (tp + fn)\n",
    "    else:\n",
    "        recall = 0\n",
    "        \n",
    "    if precision or recall:\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    \n",
    "    mape = 100*np.mean(errors / (test_labels+1))\n",
    "    accuracy = 100 - mape\n",
    "#     print('Model Performance')\n",
    "#     print('Average error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "#     print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "#     print('\\n')\n",
    "#     print(\"Precision: {:0.4f}\".format(precision))\n",
    "#     print(\"Recall: {:0.4f}\".format(recall))\n",
    "#     print(\"F1 score: {:0.4f}\".format(f1))\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.44117647058823\n",
      "0.65\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prev_best_fp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-310e58b83c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0mbest_f1_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed_base\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mdiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_diffs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_base\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_f1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_f1_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_f1_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mprecisions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-310e58b83c0d>\u001b[0m in \u001b[0;36mtrain_diffs\u001b[0;34m(train_features, train_labels, test_features, test_labels, criterion, max_depth, min_samples_split, min_samples_leaf, seed, best_f1, best_f1_seed)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#         prev_best_fp = glob('./models/20220825/best_*.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprev_best_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prev_best_fp' is not defined"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "def train_diffs(\n",
    "    train_features=None, \n",
    "    train_labels=None, \n",
    "    test_features=None, \n",
    "    test_labels=None, \n",
    "    criterion='entropy', \n",
    "    max_depth=7, #12 \n",
    "    min_samples_split=4, \n",
    "    min_samples_leaf=2, \n",
    "    seed=512,\n",
    "    best_f1=0,\n",
    "    best_f1_seed=0\n",
    "):\n",
    "    # Implement SMOTE (Sythetic Minority Oversampling Technique)\n",
    "    X = lit_features_df #df.loc[:, df.columns != 'impurity_code']\n",
    "    y = lit_labels #df.loc[:, df.columns == 'impurity_code']\n",
    "\n",
    "\n",
    "    over_sample = SMOTE(random_state=seed)\n",
    "    X_train, test_features, y_train, test_labels = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
    "    columns = X_train.columns\n",
    "\n",
    "    os_data_X, os_data_y = over_sample.fit_resample(X_train, y_train)\n",
    "\n",
    "    lit_train_features = os_data_X\n",
    "    lit_train_labels = os_data_y\n",
    "    \n",
    "#     plus_suggested_train_features = np.concatenate((os_data_X, suggested_features))\n",
    "#     plus_suggested_train_labels = np.concatenate((os_data_y, suggested_labels))\n",
    "\n",
    "    plus_suggested_train_features = lit_train_features\n",
    "    plus_suggested_train_labels = lit_train_labels\n",
    "    \n",
    "    lit_tree = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        splitter=\"best\",\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=seed,\n",
    "        max_features=None,\n",
    "    )\n",
    "    \n",
    "    plus_suggested_tree = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        splitter=\"best\",\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=seed,\n",
    "        max_features=None,\n",
    "    ) \n",
    "    \n",
    "    lit_tree.fit(lit_train_features, lit_train_labels)\n",
    "    plus_suggested_tree.fit(plus_suggested_train_features, plus_suggested_train_labels)\n",
    "\n",
    "\n",
    "    lit_eval = evaluate(lit_tree, test_features, test_labels)\n",
    "    plus_suggested_eval = evaluate(plus_suggested_tree, test_features, test_labels)\n",
    "    \n",
    "    if best_f1 < plus_suggested_eval[3]:\n",
    "        \n",
    "        print(plus_suggested_eval[0])\n",
    "        print(plus_suggested_eval[3])\n",
    "        \n",
    "        prev_best_fp = glob('./models/20220825/best_*.pkl')\n",
    "        \n",
    "        for fp in prev_best_fp:\n",
    "            os.remove(fp)\n",
    "        \n",
    "        best_f1_seed = seed\n",
    "        \n",
    "        # Visualize decision tree\n",
    "\n",
    "        # Import tools needed for visualization\n",
    "        from sklearn.tree import export_graphviz\n",
    "        import pydot\n",
    "        # Pull out one tree from the forest\n",
    "        # Export the image to a dot file\n",
    "        export_graphviz(plus_suggested_tree, out_file = '20220825_best_suggested_tree_lit.dot', feature_names = feature_list, class_names = ['pure', 'impure'], rounded = True, precision = 1)\n",
    "        # Use dot file to create a graph\n",
    "        (graph, ) = pydot.graph_from_dot_file('20220825_best_suggested_tree_lit.dot')\n",
    "        # Write graph to a png file\n",
    "#         graph.write_png('20220825_best_suggested_tree_lit.png')\n",
    "\n",
    "        importances = list(plus_suggested_tree.feature_importances_)\n",
    "        # List of tuples with variable and importance\n",
    "        feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "        # Sort the feature importances by most important first\n",
    "        feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "        # Print out the feature and importances \n",
    "        #[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "        \n",
    "        best_f1 = plus_suggested_eval[3]\n",
    "        \n",
    "#         with open(f'./models/20220825/best_{seed}_{best_f1:.2f}.pkl', 'wb') as fp:\n",
    "#             pickle.dump(plus_suggested_tree, fp)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    accs = plus_suggested_eval[0] - lit_eval[0]\n",
    "    precs = plus_suggested_eval[1] - lit_eval[1]\n",
    "    recs = plus_suggested_eval[2] - lit_eval[2]\n",
    "    f1s = plus_suggested_eval[3] - lit_eval[3]\n",
    "    \n",
    "    return (\n",
    "        accs, \n",
    "        precs, \n",
    "        recs, \n",
    "        f1s, \n",
    "        lit_eval[0], \n",
    "        lit_eval[1],\n",
    "        lit_eval[2],\n",
    "        lit_eval[3],\n",
    "        plus_suggested_eval[0], \n",
    "        plus_suggested_eval[1], \n",
    "        plus_suggested_eval[2],\n",
    "        plus_suggested_eval[3],\n",
    "        best_f1,\n",
    "        best_f1_seed\n",
    "    )\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "lit_accs = []\n",
    "lit_precisions = []\n",
    "lit_recalls = []\n",
    "lit_f1s = []\n",
    "\n",
    "plus_suggested_accs = []\n",
    "plus_suggested_precisions = []\n",
    "plus_suggested_recalls = []\n",
    "plus_suggested_f1s = []\n",
    "\n",
    "best_f1 = 0\n",
    "best_f1_seed = 0\n",
    "for seed_base in range(100):\n",
    "    diffs = train_diffs(seed=seed_base**2, best_f1=best_f1, best_f1_seed=best_f1_seed)\n",
    "    accuracies.append(diffs[0])\n",
    "    precisions.append(diffs[1])\n",
    "    recalls.append(diffs[2])\n",
    "    f1_scores.append(diffs[3])\n",
    "    \n",
    "    lit_accs.append(diffs[4])\n",
    "    lit_precisions.append(diffs[5])\n",
    "    lit_recalls.append(diffs[6])\n",
    "    lit_f1s.append(diffs[7])\n",
    "    \n",
    "    plus_suggested_accs.append(diffs[8])\n",
    "    plus_suggested_precisions.append(diffs[9])\n",
    "    plus_suggested_recalls.append(diffs[10])\n",
    "    plus_suggested_f1s.append(diffs[11])\n",
    "    \n",
    "    best_f1 = diffs[-2]\n",
    "    \n",
    "    best_f1_seed = diffs[-1]\n",
    "\n",
    "print('===========')\n",
    "print(best_f1)\n",
    "print(np.mean(accuracies))\n",
    "print(np.mean(f1_scores))\n",
    "\n",
    "print()\n",
    "print(np.mean(lit_accs))\n",
    "print(np.mean(lit_precisions))\n",
    "print(np.mean(lit_recalls))\n",
    "print(np.mean(lit_f1s))\n",
    "print()\n",
    "print(np.mean(plus_suggested_accs))\n",
    "print(np.mean(plus_suggested_precisions))\n",
    "print(np.mean(plus_suggested_recalls))\n",
    "print(np.mean(plus_suggested_f1s))\n",
    "\n",
    "print(best_f1_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from purely literature\n",
      "===============================\n",
      "93.13725490196079\n",
      "0.5714285714285715\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-541bdd2dec53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training from purely literature\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"===============================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlit_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_diffs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training with Added Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "accuracy_improvements = []\n",
    "f1_improvements = []\n",
    "\n",
    "for seed in [2**j for j in range(1,100)]:\n",
    "    print(\"Training from purely literature\")\n",
    "    print(\"===============================\")\n",
    "    lit_acc, lit_f1 = train_diffs(lit_features, lit_labels, seed=seed)\n",
    "    print()\n",
    "    print(\"Training with Added Data\")\n",
    "    print(\"========================\")\n",
    "    acc, f1 = train_diffs(features, labels, seed=seed)\n",
    "    \n",
    "    accuracy_improvements.append(acc - lit_acc)\n",
    "    f1_improvements.append(f1 - lit_f1)\n",
    "    print('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\n",
    "    print('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average improvements to accuracy: \", np.mean(accuracy_improvements))\n",
    "print(\"average improvements to f1: \", np.mean(f1_improvements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    \"criterion\" : ['gini', 'entropy'],\n",
    "    \"max_depth\" : range(2,10),\n",
    "    \"min_samples_split\" : range(2,10),\n",
    "    \"min_samples_leaf\" : range(2,5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.3, random_state=0)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    grid_tree,\n",
    "    param_grid=param_dict,\n",
    "    cv=10,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid.fit(train_features, train_labels)\n",
    "print(grid.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/20220825/best_100_0.71.pkl', 'rb') as fp:\n",
    "    saved_model = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat_name, imp in zip(feature_list, saved_model.feature_importances_.tolist()):\n",
    "    print(feat_name, imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
