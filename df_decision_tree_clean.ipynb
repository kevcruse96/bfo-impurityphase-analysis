{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define any variables that need to persist throughout\n",
    "\n",
    "Currently rerunning from the requirements cell below when changing datasets... should update this so you can get all available for later cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {\n",
    "    'mmvi_binary' : {\n",
    "        'dataframes' : {},\n",
    "        'modeling' : {},\n",
    "        'training' : {}\n",
    "    },\n",
    "    'knn_binary' : {\n",
    "        'dataframes' : {},\n",
    "        'modeling' : {},\n",
    "        'training' : {}\n",
    "    },\n",
    "    # Uncomment below if you want multilabel\n",
    "    'mmvi_multilabel' : {\n",
    "        'dataframes' : {},\n",
    "        'modeling' : {},\n",
    "        'training' : {}\n",
    "    },\n",
    "    'knn_multilabel' : {\n",
    "        'dataframes' : {},\n",
    "        'modeling' : {},\n",
    "        'training' : {}\n",
    "    }\n",
    "}\n",
    "\n",
    "rep_data = model_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Some Requirements for the Fitting\n",
    "\n",
    "Here, we define the version (date, in YYYYMMDD format) of the dataset we want to use for modeling, the imputation method for missing values ('mmvi' or 'knn'), and whether we are building a 'binary' or 'multilabel' model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.today().strftime('%Y%m%d')\n",
    "data_date = '20230828'\n",
    "smote = True\n",
    "predict_replication = True\n",
    "\n",
    "if predict_replication:\n",
    "    \n",
    "    # Hold out specific data points to test predictions on (in this case, the replication experiments)\n",
    "    data_to_hold_out = [\n",
    "        {\n",
    "            'doi' : '10.1007/s10854-013-1374-0',\n",
    "            'recipe_ids' : [3, 4, 5, 6]\n",
    "        },\n",
    "        {\n",
    "            'doi' : '10.3390/ma12091444',\n",
    "            'recipe_ids' : [258, 259, 260]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    run_dir = 'for_replication_prediction'\n",
    "    \n",
    "else:\n",
    "    data_to_hold_out = []\n",
    "    run_dir = 'general'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Switch out \"mmvi\" for \"knn\" for different imputation methods. Note the date of the file that you are importing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_modeling_fields(c, i, p):\n",
    "    \n",
    "    # Get dataframe and fill NA values\n",
    "    if c == 'master':\n",
    "        df = pd.read_csv(f'./data/bfo_df_modeling_{i}_{data_date}.csv')\n",
    "    elif c == 'replication':\n",
    "        df = pd.read_csv(f'./data/bfo_test_for_prediction_df_modeling_{i}_{data_date}.csv')\n",
    "    else:\n",
    "        df = pd.read_csv(f'./data/bfo_{c}_df_modeling_{i}_{data_date}.csv')\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # Define features dataframe\n",
    "    features_df = df.drop(\n",
    "        [\n",
    "            'Unnamed: 0', \n",
    "            'recipe_id', \n",
    "            'impurity_code', \n",
    "            'fe_rich_indicator', \n",
    "            'bi_rich_indicator'\n",
    "        ], \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    features = features_df.to_numpy()\n",
    "    \n",
    "    # Define features list\n",
    "    features_list = list(features_df.columns)\n",
    "    \n",
    "    # Define labels\n",
    "    if p == 'binary':\n",
    "        labels = df['impurity_code']\n",
    "\n",
    "    elif pred_output == 'multilabel':\n",
    "        \n",
    "        g = df[['fe_rich_indicator', 'bi_rich_indicator']]\n",
    "        columns = g.columns\n",
    "        labels = g.astype(int).dot(columns).replace({\n",
    "            \"\" : 0,\n",
    "            \"fe_rich_indicator\" : 1,\n",
    "            \"bi_rich_indicator\" : 2,\n",
    "            \"fe_rich_indicatorbi_rich_indicator\" : 3\n",
    "        }).to_numpy()\n",
    "    \n",
    "    return (\n",
    "        df, \n",
    "        {\n",
    "            'features' : features,\n",
    "            'features_list' : features_list,\n",
    "            'labels' : labels\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imputation_method, pred_output in [(key.split('_')[0], key.split('_')[1]) for key in model_data.keys()]:\n",
    "    for coverage in ['lit']: #, 'suggested']:\n",
    "        df, modeling_fields = create_df_modeling_fields(coverage, imputation_method, pred_output)\n",
    "        model_data[f'{imputation_method}_{pred_output}']['dataframes'][coverage] = df\n",
    "        model_data[f'{imputation_method}_{pred_output}']['modeling'][coverage] = modeling_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Synthesis Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "This is only applicable for binary or multiclass prediction, but not with multilabel prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smote_features(Xm, Y):\n",
    "    over_sample = SMOTE(random_state=512)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=512)\n",
    "    columns = X_train.columns\n",
    "\n",
    "    return over_sample.fit_resample(X_train, y_train)\n",
    "\n",
    "    print('length of oversampled data: ', len(os_data_X))\n",
    "    print('number of pure syntheses in oversampled data: ', len(os_data_y[os_data_y==0]))\n",
    "    print('number of impure syntheses in oversampled data: ', len(os_data_y[os_data_y==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an Evaluation Function\n",
    "\n",
    "This will output accuracy, precision, recall, and F1 score for a given model's ability to predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels, pred_output):\n",
    "    test_labels = np.array(test_labels) + 1\n",
    "    predictions = model.predict(test_features) + 1\n",
    "    \n",
    "    if pred_output == 'multilabel':\n",
    "        test_labels[test_labels > 2] = 2\n",
    "        predictions[predictions > 2] = 2\n",
    "    \n",
    "    errors = abs(predictions - (test_labels))\n",
    "    \n",
    "    report = classification_report(test_labels, predictions, output_dict=True)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Simple Prediction Function\n",
    "Get predictions for a batch of specific data points (in this case the replication attempts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_features, test_labels, pred_output):\n",
    "    test_labels = test_labels + 1\n",
    "    predictions = model.predict(test_features) + 1\n",
    "    \n",
    "    if pred_output == 'multilabel':\n",
    "        test_labels[test_labels > 2] = 2\n",
    "        predictions[predictions > 2] = 2\n",
    "    \n",
    "    errors = abs(predictions - (test_labels))\n",
    "    \n",
    "    report = classification_report(test_labels, predictions, output_dict=True)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run trainings and get evaluation\n",
    "\n",
    "Here, we are running 5 folds with 10 total repeats, getting 50 total models with 10 final evaluation metrics to average and take standard deviation\n",
    "\n",
    "First, set up models and CV frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Include an option to track the metrics with increasing training set size. \n",
    "\n",
    "\n",
    "## Check out https://stackoverflow.com/questions/42228735/scikit-learn-gridsearchcv-with-multiple-repetitions\n",
    "## for this code and discussion\n",
    "\n",
    "base_tree = DecisionTreeClassifier()\n",
    "\n",
    "param_dict = {\n",
    "    \"criterion\" : ['gini', 'entropy'],\n",
    "    \"splitter\" : ['best', 'random'],\n",
    "    \"max_depth\" : range(4,7),\n",
    "    \"min_samples_split\" : range(2,4),\n",
    "    \"min_samples_leaf\" : range(1,3),\n",
    "    \"random_state\" : [2**r for r in range(7, 13)]\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    \"F1\" : make_scorer(f1_score, average=\"micro\"),\n",
    "    \"Precision\" : make_scorer(precision_score, average=\"micro\"), \n",
    "    \"Recall\" : make_scorer(recall_score, average=\"micro\"),\n",
    "    \"Accuracy\" : make_scorer(accuracy_score)\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=5, \n",
    "    n_repeats=10\n",
    ")\n",
    "\n",
    "clf = GridSearchCV(\n",
    "    estimator=base_tree, \n",
    "    param_grid=param_dict, \n",
    "    scoring=scoring,\n",
    "    cv=cv,\n",
    "    refit=\"F1\",\n",
    "    error_score=\"raise\",\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct training size analyses, with different random seeds for error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_add_samples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    features, \n",
    "    labels, \n",
    "    test_features, \n",
    "    test_labels, \n",
    "    pred_output,\n",
    "    save=True\n",
    "):\n",
    "    clf.fit(features, labels)\n",
    "\n",
    "    best_metrics = evaluate(clf.best_estimator_, test_features, test_labels, pred_output)\n",
    "\n",
    "    best_f1 = best_metrics['macro avg']['f1-score']\n",
    "\n",
    "    if f'{len(features)}' in model_data[f'{imputation_method}_{pred_output}']['training'].keys():\n",
    "        model_data[f'{imputation_method}_{pred_output}']['training'][f'{len(features)}'].append(best_f1)\n",
    "    else:\n",
    "        model_data[f'{imputation_method}_{pred_output}']['training'][f'{len(features)}'] = [best_f1]\n",
    "    \n",
    "    if save:\n",
    "        with open(f'./models/{run_dir}/{current_date}/{imputation_method}_{pred_output}/{split_seed}/best_{coverage}_{split_seed}_{max_training_samples}_{best_f1:.2f}.pkl', 'wb') as fp:\n",
    "            pickle.dump(clf.best_estimator_, fp)\n",
    "\n",
    "        with open(f'./data/{run_dir}/{current_date}/{imputation_method}_{pred_output}/{split_seed}/best_{coverage}_{split_seed}_{max_training_samples}_{best_f1:.2f}.pkl', 'w') as fp:\n",
    "            json.dump(best_metrics, fp)\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Progress... Max Size: 10% of training data | Trial 1/6\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevcruse96/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/kevcruse96/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/kevcruse96/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/kevcruse96/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/kevcruse96/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/kevcruse96/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/kevcruse96/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/kevcruse96/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/kevcruse96/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/kevcruse96/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Progress... Max Size: 100% of training data | Trial 1/6\n",
      "\n",
      "\n",
      "\n",
      "Learning Progress... Max Size: 68% of training data | Trial 1/6\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevcruse96/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kevcruse96/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kevcruse96/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Progress... Max Size: 100% of training data | Trial 1/6\n",
      "\n",
      "\n",
      "\n",
      "Created directory ./models/for_replication_prediction/20230828/mmvi_multilabel/128\n",
      "Created directory ./data/for_replication_prediction/20230828/mmvi_multilabel/128/\n",
      "Learning Progress... Max Size: 10% of training data | Trial 1/6\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [49, 56]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e65f3a888cc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mtrial_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_training_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 train_and_evaluate(\n\u001b[0m\u001b[1;32m     96\u001b[0m                     \u001b[0mtrial_train_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0mtrial_train_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-3267ca04e336>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(features, labels, test_features, test_labels, pred_output, save)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mbest_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbest_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'macro avg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f1-score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-35fb97be1813>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, test_features, test_labels, pred_output)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \"\"\"\n\u001b[1;32m   2109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2110\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [49, 56]"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "for coverage in ['lit']: #, 'suggested']:\n",
    "    for imputation_method, pred_output in [(key.split('_')[0], key.split('_')[1]) for key in model_data.keys()]:\n",
    "        \n",
    "        iter_data = model_data[f'{imputation_method}_{pred_output}']['modeling'][coverage]\n",
    "        features_list = iter_data['features_list']\n",
    "        \n",
    "        if coverage in ['master', 'suggested'] and test_add_samples:\n",
    "            \n",
    "            add_list_of_features = []\n",
    "            add_list_of_labels = []\n",
    "            \n",
    "            list_of_features = list(iter_data['features'])\n",
    "            list_of_labels = list(iter_data['labels'])\n",
    "                       \n",
    "            for idx in range(len(iter_data['features']) - 340):\n",
    "                add_list_of_features.append(list_of_features.pop(-1))\n",
    "                add_list_of_labels.append(list_of_labels.pop(-1))             \n",
    "\n",
    "            add_features = np.array(add_list_of_features)\n",
    "            add_labels = np.array(add_list_of_labels)\n",
    "        \n",
    "            features = iter_data['features'][:340]\n",
    "            labels = iter_data['labels'][:340]\n",
    "            \n",
    "        else:\n",
    "            features = iter_data['features']\n",
    "            labels = iter_data['labels']\n",
    "            \n",
    "        if data_to_hold_out:\n",
    "            held_out_features = np.empty(features.shape)\n",
    "            held_out_labels = pd.DataFrame()\n",
    "            \n",
    "            indices_to_hold_out = [i-1 for d in data_to_hold_out for i in d['recipe_ids']]\n",
    "\n",
    "            test_size = (test_size*len(features) - len(indices_to_hold_out)) / (len(features) - len(indices_to_hold_out))\n",
    "\n",
    "            held_out_features = features[indices_to_hold_out] \n",
    "\n",
    "            features = np.delete(features, indices_to_hold_out, 0)\n",
    "\n",
    "            labels = pd.DataFrame(labels)\n",
    "            held_out_labels = labels.iloc[indices_to_hold_out]\n",
    "            labels = labels.drop(index=indices_to_hold_out)\n",
    "        \n",
    "        else:\n",
    "            held_out_features = None\n",
    "            held_out_labels = None\n",
    "        \n",
    "        for i, split_seed in enumerate([2**r for r in range(7, 8)]):\n",
    "\n",
    "            (\n",
    "                train_features, \n",
    "                test_features, \n",
    "                train_labels, \n",
    "                test_labels\n",
    "            ) = train_test_split(\n",
    "                features, \n",
    "                labels, \n",
    "                test_size=test_size, \n",
    "                stratify=labels, # Include this to ensure that label balance in test set is proportionate to the full dataset \n",
    "                random_state=split_seed\n",
    "            )\n",
    "            \n",
    "            print()\n",
    "            \n",
    "            if data_to_hold_out:\n",
    "                if held_out_features.any():\n",
    "                    test_features = np.vstack([test_features, held_out_features])\n",
    "\n",
    "                if held_out_labels.any()[0]: # Quite weird that the indexing is needed... for some reason held_out_labels.any() is returned as another series\n",
    "                    test_labels = np.concatenate([test_labels, held_out_labels])\n",
    "                \n",
    "            if not os.path.exists(f'./models/{run_dir}/{current_date}/{imputation_method}_{pred_output}/{split_seed}'):\n",
    "                os.makedirs(f'./models/{run_dir}/{current_date}/{imputation_method}_{pred_output}/{split_seed}')\n",
    "                print(f'Created directory ./models/{run_dir}/{current_date}/{imputation_method}_{pred_output}/{split_seed}')\n",
    "\n",
    "            if not os.path.exists(f'./data/{run_dir}/{current_date}/{imputation_method}_{pred_output}/{split_seed}/'):\n",
    "                os.makedirs(f'./data/{run_dir}/{current_date}/{imputation_method}_{pred_output}/{split_seed}/')\n",
    "                print(f'Created directory ./data/{run_dir}/{current_date}/{imputation_method}_{pred_output}/{split_seed}/')\n",
    "                \n",
    "            if not os.path.exists(f'./data/{run_dir}/dt_learning_curves/{current_date}/'):\n",
    "                os.makedirs(f'./data/{run_dir}/dt_learning_curves/{current_date}/')\n",
    "                print(f'Created directory ./data/{run_dir}/dt_learning_curves/{current_date}/')\n",
    "\n",
    "            for train_size in np.linspace(0.1, 1, 12):\n",
    "\n",
    "                print(f'Learning Progress... Max Size: {int(np.ceil(train_size*100))}% of training data | Trial {i+1}/{len([2**r for r in range(7, 13)])}', end='\\r')\n",
    "\n",
    "                max_training_samples = int(np.ceil(train_size*len(train_features)))\n",
    "\n",
    "                trial_train_features = train_features[:max_training_samples]\n",
    "                trial_train_labels = train_labels[:max_training_samples]\n",
    "                \n",
    "                train_and_evaluate(\n",
    "                    trial_train_features, \n",
    "                    trial_train_labels, \n",
    "                    test_features, \n",
    "                    test_labels,\n",
    "                    pred_output\n",
    "                )\n",
    "                \n",
    "                # If we are testing additional, suggested samples in training set,\n",
    "                # then do one more training and evaluation with the additional features\n",
    "                # and labels added to the training set\n",
    "                if train_size == 1 and test_add_samples:\n",
    "                    trial_train_add_features = np.concatenate((trial_train_features, add_features))\n",
    "                    trial_train_add_labels = np.concatenate((trial_train_labels, add_labels))\n",
    "                    \n",
    "                    train_and_evaluate(trial_train_add_features, trial_train_add_labels, test_features, test_labels)\n",
    "\n",
    "            saved_features = model_data[f'{imputation_method}_{pred_output}']['modeling']\n",
    "            saved_training_metrics = model_data[f'{imputation_method}_{pred_output}']['training']\n",
    "            \n",
    "            with open(f'./data/{run_dir}/dt_learning_curves/{current_date}/{coverage}_thru_{imputation_method}_{pred_output}_{split_seed}.json', 'w') as fp:\n",
    "                json.dump({\n",
    "                    k : {\n",
    "                        'modeling' : {\n",
    "                            l : list(model_data[k]['modeling'][l]) for \n",
    "                            l in model_data[k]['modeling']\n",
    "                        }, \n",
    "                        'training' : model_data[k]['training']\n",
    "                    } for k in model_data.keys()}, fp)\n",
    "\n",
    "            print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions over the replication experiments\n",
    "\n",
    "If available, run predictions over suggested experiments to replicate published results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, grab the best performing models from each \n",
    "os.chdir('/Users/kevcruse96/PycharmProjects/bfo-impurityphase-analysis/models/20230526/')\n",
    "\n",
    "for k in rep_data.keys():\n",
    "    best_models = []\n",
    "    for seed in [2**r for r in range(7, 13)]:\n",
    "        max_f1 = 0\n",
    "        best_split = None\n",
    "        for file in list(glob(f'./{k}/{seed}/*.pkl')):\n",
    "            f1 = int(file.split('.')[2])\n",
    "            if f1 > max_f1:\n",
    "                max_f1 = f1\n",
    "                best_split = file.split('_')[4]\n",
    "        best_models.append(f'./{k}/{seed}/best_lit_{seed}_{best_split}_0.{max_f1}.pkl')\n",
    "    \n",
    "    rep_data[k]['modeling']['best_models'] = best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmvi_binary\n",
      "[0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 1 1 1 1 1 1 0 0 0 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "knn_binary\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "mmvi_multilabel\n",
      "[0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      "[0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      "[0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 2 2 0 2 2 2 2 2 2 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "knn_multilabel\n",
      "[1 1 1 0 0 0 0 1 1 1 0 0 0 0 1]\n",
      "[1 1 1 0 0 0 0 1 1 1 0 0 0 0 1]\n",
      "[0 0 0 0 1 1 1 1 1 1 0 0 0 1 1]\n",
      "[2 2 2 0 2 2 2 2 2 2 0 0 0 0 0]\n",
      "[0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      "[0 0 0 0 2 2 2 2 2 2 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import pydotplus\n",
    "\n",
    "coverage = 'replication'\n",
    "data_date = 20230526\n",
    "\n",
    "os.chdir('/Users/kevcruse96/PycharmProjects/bfo-impurityphase-analysis/')\n",
    "mmvi_rep_df = pd.read_csv('./data/bfo_test_for_prediction_df_modeling_mmvi_20230526.csv')\n",
    "knn_rep_df = pd.read_csv('./data/bfo_test_for_prediction_df_modeling_knn_20230526.csv')   \n",
    "\n",
    "\n",
    "for imputation_method, pred_output in [(key.split('_')[0], key.split('_')[1]) for key in rep_data.keys()]:\n",
    "    df, modeling_fields = create_df_modeling_fields(coverage, imputation_method, pred_output)\n",
    "    rep_data[f'{imputation_method}_{pred_output}']['dataframes'][coverage] = df\n",
    "    rep_data[f'{imputation_method}_{pred_output}']['modeling'][coverage] = modeling_fields\n",
    "\n",
    "os.chdir('/Users/kevcruse96/PycharmProjects/bfo-impurityphase-analysis/models/20230526/')\n",
    "\n",
    "importances = []\n",
    "\n",
    "for k in rep_data.keys():\n",
    "    print(k)\n",
    "    for m in rep_data[k]['modeling']['best_models']:\n",
    "        model = pickle.load(open(m, 'rb'))\n",
    "        importances.append(model.feature_importances_)\n",
    "        prediction = model.predict(rep_data[k]['modeling']['replication']['features'])\n",
    "        print(prediction)\n",
    "        \n",
    "        for idx in range(0, prediction.shape[0]):\n",
    "\n",
    "            dot_data = tree.export_graphviz(model, out_file=None,\n",
    "                                            feature_names=rep_data[k]['modeling']['replication']['features_list'],\n",
    "                                            class_names=['phase pure', 'phase impure', 'phase impure', 'phase_impure'],\n",
    "                                            filled=True, rounded=True,\n",
    "                                            special_characters=True)\n",
    "            graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "            # empty all nodes, i.e.set color to white and number of samples to zero\n",
    "            for node in graph.get_node_list():\n",
    "                if node.get_attributes().get('label') is None:\n",
    "                    continue\n",
    "                if 'samples = ' in node.get_attributes()['label']:\n",
    "                    labels = node.get_attributes()['label'].split('<br/>')\n",
    "                    for i, label in enumerate(labels):\n",
    "                        if label.startswith('samples = '):\n",
    "                            labels[i] = 'samples = 0'\n",
    "                    node.set('label', '<br/>'.join(labels))\n",
    "                    node.set_fillcolor('white')\n",
    "\n",
    "            samples = rep_data[k]['modeling']['replication']['features'][idx:idx+1]\n",
    "            decision_paths = model.decision_path(samples)\n",
    "\n",
    "            for decision_path in decision_paths:\n",
    "                for n, node_value in enumerate(decision_path.toarray()[0]):\n",
    "                    if node_value == 0:\n",
    "                        continue\n",
    "                    node = graph.get_node(str(n))[0]            \n",
    "                    node.set_fillcolor('green')\n",
    "                    labels = node.get_attributes()['label'].split('<br/>')\n",
    "                    for i, label in enumerate(labels):\n",
    "                        if label.startswith('samples = '):\n",
    "                            labels[i] = 'samples = {}'.format(int(label.split('=')[1]) + 1)\n",
    "\n",
    "                    node.set('label', '<br/>'.join(labels))\n",
    "\n",
    "            filename = f'./{k}/exp{idx}_{m.split(\"_\")[4]}_{m.split(\".\")[2]}_tree.png'\n",
    "            graph.write_png(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 'low_coating_rpm'),\n",
      " (0.0, 'n2_atm'),\n",
      " (0.0, 'chem_pca-c3'),\n",
      " (0.0, 'chem_pca-c8'),\n",
      " (0.0, 'chem_pca-c10'),\n",
      " (0.0, 'chem_pca-c13'),\n",
      " (0.0, 'chem_pca-c20'),\n",
      " (0.0, 'chem_pca-c23'),\n",
      " (0.000473388785517213, 'age_temp_degC'),\n",
      " (0.0007008829004847653, 'chem_pca-c14'),\n",
      " (0.0008375802189505473, 'chem_pca-c9'),\n",
      " (0.0010582637453769258, 'separate_hydrolysis'),\n",
      " (0.0012918449619900513, 'chem_pca-c7'),\n",
      " (0.0014865395775896493, 'chem_pca-c15'),\n",
      " (0.0017902703161378254, 'chem_pca-c27'),\n",
      " (0.0022825728996719833, 'low_coating_time_sec'),\n",
      " (0.003576392613955483, 'chem_pca-c2'),\n",
      " (0.0035836270552499273, 'final_prebake_degC'),\n",
      " (0.003974878230918387, 'chem_pca-c17'),\n",
      " (0.005495200616404229, 'chem_pca-c30'),\n",
      " (0.0055680053432665124, 'chem_pca-c4'),\n",
      " (0.005568755431486494, 'o2_atm'),\n",
      " (0.005814839157354975, 'chem_pca-c26'),\n",
      " (0.005989263391977876, 'dry_degC'),\n",
      " (0.006287185303527304, 'final_prebake_time_min'),\n",
      " (0.007070391235210061, 'layer_prebake_degC'),\n",
      " (0.007988523971650462, 'chem_pca-c19'),\n",
      " (0.008651934528241768, 'chem_pca-c11'),\n",
      " (0.00877576888872814, 'chem_pca-c21'),\n",
      " (0.009172052211981106, 'chem_pca-c29'),\n",
      " (0.009365634365634366, 'chem_pca-c18'),\n",
      " (0.009404167073148373, 'chem_pca-c22'),\n",
      " (0.00992122586336293, 'chem_pca-c1'),\n",
      " (0.010257843688480872, 'chem_pca-c12'),\n",
      " (0.010465595390173546, 'age_days'),\n",
      " (0.01060893628699644, 'nitrate_precs'),\n",
      " (0.010759505628763296, 'high_coating_rpm'),\n",
      " (0.010952687642650763, 'air_atm'),\n",
      " (0.011841311987582427, 'layer_prebake_time_min'),\n",
      " (0.012320871746006412, 'dry_time_min'),\n",
      " (0.015098218527194125, 'final_annealing_time_hr'),\n",
      " (0.015203553950682328, 'high_coating_time_sec'),\n",
      " (0.015741120032497186, 'chem_pca-c24'),\n",
      " (0.0189360806107339, 'layer_annealing_time_min'),\n",
      " (0.02280010642638917, 'chem_pca-c28'),\n",
      " (0.02322925840076524, 'chem_pca-c6'),\n",
      " (0.027653907582047017, 'chem_pca-c5'),\n",
      " (0.03095912940977774, 'chem_pca-c16'),\n",
      " (0.03252458587679177, 'chem_pca-c25'),\n",
      " (0.03453738583753179, 'layers'),\n",
      " (0.03531478986411693, 'precursor_concentration'),\n",
      " (0.035599688186897606, 'pH'),\n",
      " (0.06532507221877798, 'stirring_time_hr'),\n",
      " (0.08325023729432715, 'stirring_temp_degC'),\n",
      " (0.08781705570496924, 'bi_fe_ratio'),\n",
      " (0.087858190038334, 'final_annealing_degC'),\n",
      " (0.1648156789796957, 'layer_annealing_degC')]\n"
     ]
    }
   ],
   "source": [
    "feature_importances_tuples = zip(np.mean(importances, axis=0), rep_data[k]['modeling']['replication']['features_list'])\n",
    "sorted_feature_importances_tuples = sorted(feature_importances_tuples, key=lambda x:x[0])\n",
    "\n",
    "pprint(sorted_feature_importances_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a specific tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_f1)\n",
    "# Visualize best decision tree\n",
    "\n",
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Pull out one tree from the forest\n",
    "# Export the image to a dot file\n",
    "export_graphviz(clf.best_estimator_, out_file = f'{current_date}_best_tree_lit_{seed}_{imputation_method}_{pred_output}.dot', feature_names = lit_feature_list, class_names = ['phase pure', 'fe-rich impurity', 'bi-rich impurity', 'both impurities'], rounded = True, precision = 1)\n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file(f'{current_date}_best_tree_lit_{seed}_{imputation_method}_{pred_output}.dot')\n",
    "# Write graph to a png file\n",
    "graph.write_png(f'{current_date}_best_tree_lit_{seed}_{imputation_method}_{pred_output}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Function for Training\n",
    "\n",
    "Function should optionally train only using the literature dataset or both literature and suggested experiments along with the performance differences between the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    X,\n",
    "    y,\n",
    "    criterion='entropy', \n",
    "    max_depth=7, #12 \n",
    "    min_samples_split=4, \n",
    "    min_samples_leaf=2, \n",
    "    seed=512\n",
    "):\n",
    "    # Since you're already stratifying, maybe just implement a k-folds CV framework? \n",
    "    \n",
    "    X_train, test_features, y_train, test_labels = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
    "\n",
    "    if pred_output=='binary' and smote:\n",
    "        X, y = get_smote_features(X_train, y_train)\n",
    "        \n",
    "    base_tree = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        splitter=\"best\",\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=seed,\n",
    "        max_features=None,\n",
    "    )\n",
    "    \n",
    "    base_tree.fit(X_train, y_train)\n",
    "    base_eval = evaluate(base_tree, test_features, test_labels)\n",
    "    \n",
    "    pprint(base_eval)\n",
    "    print()\n",
    "\n",
    "    return base_tree, base_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_improvements = []\n",
    "f1_improvements = []\n",
    "\n",
    "for seed in [2**j for j in range(1,100)]:\n",
    "    print(\"Training from purely literature\")\n",
    "    print(\"===============================\")\n",
    "    lit_acc, lit_f1 = train_diffs(lit_features, lit_labels, seed=seed)\n",
    "    print()\n",
    "    print(\"Training with Added Data\")\n",
    "    print(\"========================\")\n",
    "    acc, f1 = train_diffs(features, labels, seed=seed)\n",
    "    \n",
    "    accuracy_improvements.append(acc - lit_acc)\n",
    "    f1_improvements.append(f1 - lit_f1)\n",
    "    print('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\n",
    "    print('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average improvements to accuracy: \", np.mean(accuracy_improvements))\n",
    "print(\"average improvements to f1: \", np.mean(f1_improvements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    \"criterion\" : ['gini', 'entropy'],\n",
    "    \"max_depth\" : range(2,10),\n",
    "    \"min_samples_split\" : range(2,10),\n",
    "    \"min_samples_leaf\" : range(2,5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.3, random_state=0)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    grid_tree,\n",
    "    param_grid=param_dict,\n",
    "    cv=10,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid.fit(train_features, train_labels)\n",
    "print(grid.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/20220825/best_100_0.71.pkl', 'rb') as fp:\n",
    "    saved_model = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat_name, imp in zip(feature_list, saved_model.feature_importances_.tolist()):\n",
    "    print(feat_name, imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
